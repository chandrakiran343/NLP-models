{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32033\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "words = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10, 'k': 11, 'l': 12, 'm': 13, 'n': 14, 'o': 15, 'p': 16, 'q': 17, 'r': 18, 's': 19, 't': 20, 'u': 21, 'v': 22, 'w': 23, 'x': 24, 'y': 25, 'z': 26, '.': 0}\n",
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chs = []\n",
    "a = torch.zeros([27,27], dtype=torch.int32)\n",
    "mapping = sorted(list(set(\"\".join(words))))\n",
    "\n",
    "chartoint = {x:i+1 for i,x in enumerate(mapping)} \n",
    "chartoint[\".\"] = 0\n",
    "inttochar = {x:i for i,x in chartoint.items()}  \n",
    "print(chartoint)\n",
    "print(inttochar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    chs = [\".\"] + list(w) + [\".\"]\n",
    "\n",
    "    for ch1 , ch2 in zip(chs, chs[1:]):\n",
    "        pointer1 = chartoint[ch1]\n",
    "        pointer2 = chartoint[ch2]\n",
    "\n",
    "        a[pointer1, pointer2] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "plt.imshow(a, cmap=\"Blues\")\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = inttochar[i] + inttochar[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\",color=\"gray\")\n",
    "        plt.text(j, i, a[i,j].item(), ha=\"center\", va=\"top\",color=\"gray\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(666996)\n",
    "p = (a+1).float()\n",
    "\n",
    "print(p.sum(1))\n",
    "p /= p.sum(1, keepdim=True)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    string = []\n",
    "    ix = 0\n",
    "    out = []\n",
    "    while True:\n",
    "\n",
    "        row = p[ix]\n",
    "        sample = torch.multinomial(row, num_samples=1,replacement=True, generator=g).item()\n",
    "        out.append(inttochar[sample])\n",
    "        \n",
    "        \n",
    "        ix = sample\n",
    "        if ix == 0:\n",
    "            string.append(out)\n",
    "            break\n",
    "    print(\"\".join(string[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_log_prob = 0.0\n",
    "n = 0\n",
    "for w in words:\n",
    "    ch = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2 in zip(ch, ch[1:]):\n",
    "\n",
    "        ix1 = chartoint[ch1]\n",
    "        ix2 = chartoint[ch2]\n",
    "\n",
    "        log_prob = torch.log(p[ix1,ix2])\n",
    "        n+=1\n",
    "        total_log_prob += log_prob\n",
    "\n",
    "nll = -total_log_prob\n",
    "nll /= n\n",
    "nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([164015, 3, 27])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training set for the neural network\n",
    "\n",
    "ys, xs = [], []\n",
    "elements = 0\n",
    "for w in words[:]:\n",
    "    if(len(w)<3):\n",
    "        continue\n",
    "    word = [\".\"] + list(w) + [\".\"]\n",
    "    for ch1, ch2,ch3,ch4 in zip(word, word[1:],word[2:],word[3:]):\n",
    "        elements+=1\n",
    "        intx1 = chartoint[ch1]\n",
    "        intx2 = chartoint[ch2]\n",
    "        intx3 = chartoint[ch3]\n",
    "        intx4 = chartoint[ch4]\n",
    "        xs.append([intx1,intx2,intx3])\n",
    "        ys.append(intx4)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "xenc.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(666996)\n",
    "\n",
    "weights = torch.randn((27,27), requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count torch.Size([164015, 3, 27])\n",
      "probs torch.Size([164015, 3, 27])\n",
      "probs2 torch.Size([164015, 27])\n",
      "tensor(2.3139, grad_fn=<NegBackward0>)\n",
      "tensor([[-1.6884e+00,  1.2937e+00,  1.5912e-01,  3.5266e-01, -7.9957e-02,\n",
      "          1.3072e+00, -6.9457e-01, -1.1177e-01,  9.8850e-01,  5.2143e-02,\n",
      "         -9.4931e-02, -5.1328e-01,  2.1666e-01, -2.5192e+00,  3.6388e-01,\n",
      "          1.3021e+00, -7.1283e-01,  4.7346e-01,  1.5441e+00, -1.0345e+00,\n",
      "          4.1473e-01, -8.5902e-01, -5.2727e-01, -4.7250e-01, -1.5372e+00,\n",
      "          2.5493e-01,  4.4675e-01],\n",
      "        [ 3.7057e+00, -1.8810e+00,  1.0507e+00, -1.2255e-02,  1.2094e+00,\n",
      "         -1.0060e+00, -8.2457e-01, -5.5831e-01, -3.6949e-01,  2.4656e+00,\n",
      "          8.3287e-01,  8.9706e-02, -9.3172e-01,  1.2616e+00,  1.2160e+00,\n",
      "         -2.5273e-01, -2.2852e+00, -5.9606e-01,  1.0906e+00, -2.0909e-01,\n",
      "         -1.7364e+00, -3.6437e-01,  5.9333e-01, -8.8205e-01,  8.6381e-01,\n",
      "         -3.6853e-01, -4.8722e-01],\n",
      "        [-8.7541e-01,  8.1060e-01,  3.2292e-02,  9.4822e-01,  7.1794e-01,\n",
      "         -6.2573e-01,  1.2803e+00,  7.3656e-01, -1.2321e+00, -7.8214e-01,\n",
      "         -3.2338e-01, -1.3074e+00, -5.2978e-01, -1.9264e+00,  7.6411e-01,\n",
      "         -1.0048e+00,  8.6410e-01,  5.0443e-01, -2.9087e-01,  1.3103e+00,\n",
      "          8.2275e-01,  1.7573e-01,  1.1779e+00,  7.4563e-01, -1.2703e+00,\n",
      "          1.2748e-01,  9.5440e-02],\n",
      "        [-2.1195e-02,  2.8796e-01,  7.9157e-01,  2.8027e-01,  1.5319e+00,\n",
      "         -1.4450e+00, -1.0593e+00, -2.3156e+00, -2.2953e-02, -9.5186e-01,\n",
      "          4.9913e-01,  2.5639e+00,  2.0764e-01,  2.4073e+00, -1.1375e+00,\n",
      "          1.0378e+00,  3.8881e-01,  7.4365e-01, -2.4236e+00,  6.8018e-01,\n",
      "         -1.2284e+00,  5.0728e-01,  9.9420e-01,  3.4798e-01,  2.2892e-01,\n",
      "          3.1552e-01,  5.4123e-02],\n",
      "        [-3.8345e-01,  1.7145e+00, -4.0483e-01, -1.0736e-01,  4.8094e-01,\n",
      "         -8.5054e-01,  1.3889e+00, -5.0420e-01,  9.1269e-01,  1.5207e-01,\n",
      "         -1.2880e+00,  9.1120e-01,  1.4821e-01, -2.6912e-01, -8.1367e-01,\n",
      "         -9.6350e-01,  4.0802e-01,  1.6150e+00, -8.5480e-01, -8.3496e-01,\n",
      "          2.3846e-01, -2.4157e-01,  7.8080e-01,  3.9493e-01,  3.4369e-01,\n",
      "          1.5502e+00, -9.8302e-01],\n",
      "        [ 9.8917e-01, -8.0647e-01, -9.5105e-01,  9.6745e-01,  8.4584e-01,\n",
      "         -2.3952e-01, -2.9739e-01,  7.3713e-01, -2.2346e-01, -1.2256e+00,\n",
      "         -4.1149e-01, -1.0882e+00, -3.2388e-01, -6.7231e-01, -1.5420e+00,\n",
      "         -6.2802e-01, -1.7107e+00,  5.4948e-01,  1.2930e+00, -8.5919e-02,\n",
      "          3.8048e-01,  1.7002e-01, -6.3732e-01,  5.4271e-01,  4.5892e-01,\n",
      "         -2.5315e-01,  2.6527e-01],\n",
      "        [ 2.8031e-02, -1.7511e+00, -2.0715e-01,  3.3912e-01, -1.5123e-01,\n",
      "         -1.0437e+00,  1.6392e+00,  2.7391e-01,  2.0800e+00,  2.0766e-01,\n",
      "          1.2541e+00,  1.3097e+00, -1.5048e+00, -7.5453e-02, -4.3647e-01,\n",
      "         -9.0627e-01,  6.9809e-01, -1.5495e-01, -6.5490e-01, -2.3386e+00,\n",
      "         -6.4466e-01,  3.7147e-01,  1.0860e+00, -3.1610e-01, -2.1491e+00,\n",
      "          1.3089e+00, -1.4232e-01],\n",
      "        [ 1.3779e+00,  8.8560e-01, -7.2970e-02, -1.2433e+00,  1.0544e+00,\n",
      "         -6.5318e-01, -2.7528e+00,  7.1321e-01, -9.4174e-01,  3.3392e-01,\n",
      "          2.8844e+00, -1.5969e+00, -7.3763e-01,  2.2270e-01, -1.8418e-01,\n",
      "         -1.4024e-01,  1.1259e+00, -2.0382e+00,  6.4273e-01,  1.5887e+00,\n",
      "         -2.2379e+00, -4.0924e-01,  8.7621e-01,  1.1621e+00,  1.3288e+00,\n",
      "          1.1997e-01, -1.9786e+00],\n",
      "        [ 5.9998e-01,  9.9305e-01,  1.4691e+00, -5.5235e-01, -2.8169e-01,\n",
      "         -9.2777e-01,  1.5182e-01, -2.0844e-01,  1.4761e+00,  9.5050e-01,\n",
      "         -8.6277e-01, -4.9344e-01,  1.1996e+00, -1.3295e+00,  9.0351e-01,\n",
      "         -1.6450e+00, -7.9038e-01,  1.5850e-01, -1.1820e+00,  5.7855e-01,\n",
      "         -4.0939e-01, -2.3104e-01, -5.7692e-01,  7.2316e-02, -6.7079e-02,\n",
      "          1.2683e+00,  1.5313e+00],\n",
      "        [ 2.0047e+00,  1.7667e+00,  4.0032e-01, -1.4015e+00, -3.7493e-01,\n",
      "         -1.6106e+00, -9.2212e-02, -2.1991e+00,  7.9213e-01, -5.5836e-01,\n",
      "         -9.7344e-01, -1.2862e-01, -2.5930e-01,  3.6961e-01,  4.0172e-02,\n",
      "         -1.0442e+00,  2.7300e-01, -5.3908e-01, -1.5049e+00,  5.4829e-01,\n",
      "         -8.7151e-01, -1.5870e-01,  1.6850e-01, -7.3595e-01, -8.4883e-01,\n",
      "         -1.2767e+00,  4.7197e-01],\n",
      "        [-1.3888e+00,  1.3779e-01, -2.1927e+00, -4.5995e-01,  6.8393e-01,\n",
      "          7.7414e-01,  3.5429e-02,  1.1248e+00, -1.2761e+00, -1.3343e+00,\n",
      "         -1.1278e+00, -3.1866e-01, -1.8028e-01,  1.8091e+00, -2.2990e+00,\n",
      "         -1.1632e+00,  6.9572e-02, -3.3429e-01, -1.3724e+00,  1.6340e-01,\n",
      "         -2.1820e+00, -6.3259e-01,  9.2947e-01, -9.4602e-01, -2.0365e+00,\n",
      "          5.7784e-01, -7.1540e-01],\n",
      "        [-9.1193e-01, -1.6009e-03, -8.0103e-01, -9.9899e-02,  2.1624e-01,\n",
      "         -3.6474e-01,  1.9714e+00, -3.6385e-01,  1.4753e-01,  5.8829e-01,\n",
      "          1.6636e+00, -4.6795e-01,  1.5445e-01,  3.9532e-01,  6.5888e-02,\n",
      "          5.7310e-01, -8.1331e-01, -6.2599e-02,  3.5694e-01,  2.7748e-02,\n",
      "         -2.6937e-01,  1.1268e+00, -5.6576e-01, -6.2897e-01, -1.0654e+00,\n",
      "          9.8618e-03,  9.3147e-03],\n",
      "        [ 1.3197e+00, -2.1182e+00,  1.2324e+00, -2.7698e-01,  2.1667e-01,\n",
      "          9.5377e-01, -3.9427e-01, -2.0505e-01,  6.3312e-01,  7.1258e-01,\n",
      "          9.1762e-02,  1.3317e+00,  3.2362e-01,  8.4881e-01,  7.7065e-01,\n",
      "          9.8560e-01,  1.3806e-01,  9.9983e-01, -4.5343e-01,  4.1026e-01,\n",
      "          3.4218e-01,  5.7523e-02, -8.4418e-01, -5.6050e-01, -3.6014e-01,\n",
      "          1.6226e-01, -1.6905e-01],\n",
      "        [-9.8981e-01, -4.7654e-01,  2.6445e-01,  4.8069e-01,  6.8928e-01,\n",
      "          3.7832e-01, -1.6786e+00,  9.9121e-01, -1.6588e+00,  1.5582e-01,\n",
      "          1.8666e-01,  5.3293e-01, -7.4422e-02,  5.3505e-01,  1.2657e+00,\n",
      "         -3.3976e-01,  5.9970e-02, -2.3271e+00, -3.8177e-01, -1.4609e-01,\n",
      "          1.9611e+00, -1.7146e+00,  1.0532e+00,  3.4028e-01, -9.9239e-01,\n",
      "          6.5828e-01,  5.4463e-01],\n",
      "        [ 1.0852e-01,  2.2137e-01,  7.3409e-01, -4.4270e-01, -1.6039e-01,\n",
      "          7.9914e-01, -5.2089e-01,  5.0142e-01,  8.2744e-01,  9.5175e-02,\n",
      "         -5.1518e-01, -3.5982e-01, -1.0231e+00, -4.8723e-01,  1.0000e+00,\n",
      "          4.6903e-01, -3.4591e-01,  1.3542e+00,  1.6691e-01, -3.7995e-02,\n",
      "          1.3720e+00,  5.0183e-01, -9.5268e-01,  6.9136e-01,  9.0469e-01,\n",
      "          2.7545e-01, -7.4884e-02],\n",
      "        [ 8.8767e-01, -1.6842e+00,  3.8365e-01, -1.3139e+00,  9.5020e-01,\n",
      "          3.4450e-01,  1.0612e+00, -8.5186e-02, -3.0839e-01, -1.6792e+00,\n",
      "         -1.3620e+00,  7.8208e-01, -1.9509e+00,  1.6120e-01, -8.3993e-01,\n",
      "          6.6202e-02,  3.4454e-01,  1.6464e-01,  1.7708e-01,  5.7795e-01,\n",
      "         -3.1868e-01,  9.1782e-01, -1.5254e+00, -8.5517e-01,  2.1576e+00,\n",
      "          7.9993e-01,  1.1398e+00],\n",
      "        [ 1.9042e-01,  9.8201e-01, -1.2385e+00, -6.3905e-01,  9.0002e-01,\n",
      "         -2.2131e-01, -6.3480e-01, -2.6413e-01,  1.1130e+00,  2.1819e+00,\n",
      "         -6.0543e-02, -4.3408e-01,  6.5721e-01, -5.8176e-01, -5.9903e-01,\n",
      "          5.8357e-01,  5.7304e-01, -4.8495e-02,  6.5364e-01, -1.9276e+00,\n",
      "          1.3137e+00, -5.0094e-01,  1.1458e+00,  3.2746e-01, -2.4856e-01,\n",
      "          5.4953e-01,  1.0885e-01],\n",
      "        [ 9.6590e-01,  5.1438e-02, -6.0988e-01, -1.6998e-02,  7.7044e-01,\n",
      "         -1.3391e-01, -2.9908e-02, -6.8543e-01,  1.4503e+00,  1.1974e+00,\n",
      "          4.5090e-02, -6.3029e-01, -9.2624e-01,  1.6047e+00,  1.0062e+00,\n",
      "         -2.5046e-01,  5.1199e-01, -4.9189e-01, -4.7897e-02,  5.5366e-01,\n",
      "         -3.3797e-01,  4.5441e-01,  7.4597e-01, -1.1017e+00,  1.6169e+00,\n",
      "         -1.3030e+00, -1.2229e+00],\n",
      "        [ 1.0279e+00, -6.3599e-01,  7.3150e-01, -9.5213e-01,  1.0601e+00,\n",
      "         -5.6529e-01, -6.5626e-01,  2.5282e-01,  7.1701e-01,  9.4154e-01,\n",
      "          1.0073e+00, -2.0676e-01,  1.3370e+00, -5.3111e-01,  1.5852e+00,\n",
      "         -2.0702e-01, -1.2709e+00,  1.5312e+00,  2.0779e+00,  1.3758e+00,\n",
      "         -2.3934e-02,  7.7286e-01, -3.5708e-01,  8.4321e-01,  7.1564e-01,\n",
      "         -2.4490e-01,  8.7133e-01],\n",
      "        [ 2.8670e+00, -1.5127e-01,  4.3664e-01, -8.9133e-01,  4.6926e-01,\n",
      "         -1.6512e+00, -6.9611e-01,  8.6083e-01,  1.2590e+00, -1.0463e+00,\n",
      "          5.4270e-01, -3.4832e-01, -1.7096e+00,  2.8691e-01,  1.0145e+00,\n",
      "          8.4749e-01, -2.0874e+00,  5.4149e-01,  2.0526e+00,  1.2039e+00,\n",
      "          8.3591e-01,  6.8387e-01,  1.3057e-01,  2.2522e-02,  2.1048e-01,\n",
      "          4.8630e-01, -1.1664e+00],\n",
      "        [-1.1367e+00,  1.8125e+00,  1.6616e-01, -5.7472e-01, -1.1268e+00,\n",
      "          1.4427e+00,  5.0022e-01,  7.2416e-01,  4.4141e-02,  3.7507e-01,\n",
      "          7.5058e-01, -5.8731e-01, -1.1147e-01, -5.5489e-01, -8.5741e-01,\n",
      "          1.4706e+00,  1.8638e-01,  1.2026e+00,  2.2090e-01,  3.2650e-02,\n",
      "          1.7056e+00, -8.8086e-01,  8.1062e-01, -1.2831e+00,  1.2995e+00,\n",
      "          1.3308e+00,  3.9595e-01],\n",
      "        [-6.2425e-02,  6.7294e-01, -1.4499e-01, -8.3149e-01, -1.9202e-01,\n",
      "         -2.1177e-01,  8.0575e-01,  5.3330e-01, -1.6088e+00, -6.8161e-01,\n",
      "         -6.2614e-02, -3.9625e-01,  7.8038e-01, -9.3344e-02, -9.5621e-01,\n",
      "         -5.9280e-02,  2.3274e+00,  1.8614e+00, -3.5987e-01,  1.1220e-03,\n",
      "         -9.8165e-02, -2.1433e+00, -4.3241e-01, -2.0177e+00, -5.0351e-01,\n",
      "          4.8977e-01,  3.2335e-01],\n",
      "        [ 5.4500e-01, -4.9289e-02, -8.1672e-01,  1.2251e+00,  2.5772e-01,\n",
      "         -4.8486e-01, -1.0710e+00,  1.6546e+00,  2.4250e+00,  7.8131e-01,\n",
      "          1.6136e+00,  8.2272e-01,  1.8158e-01, -1.1305e+00,  1.0205e+00,\n",
      "         -4.5408e-01,  8.0764e-02,  1.2151e+00,  3.6391e-01,  5.4232e-01,\n",
      "         -5.8837e-01,  3.9590e-01, -4.9778e-01, -7.6487e-02, -6.5229e-01,\n",
      "         -8.1700e-01,  1.7587e-01],\n",
      "        [ 1.1875e+00, -6.3043e-01,  1.5169e-02,  2.1214e+00, -6.4897e-01,\n",
      "          1.9621e+00, -4.4277e-01, -6.8936e-01,  4.7592e-01, -1.5482e+00,\n",
      "          9.6122e-01, -1.7516e+00, -4.5708e-02, -3.7441e-01,  7.5470e-01,\n",
      "          5.7492e-01, -9.1033e-01,  3.1419e-01, -9.1843e-03,  1.6767e+00,\n",
      "          2.3796e+00,  2.3818e-01, -4.2679e-01,  6.8819e-01, -1.9315e-01,\n",
      "          1.0365e+00, -1.2495e+00],\n",
      "        [-7.8814e-01, -1.1069e+00,  1.6621e-01, -1.2376e+00, -2.6521e+00,\n",
      "         -8.6355e-01,  3.5185e-01,  9.9177e-02,  1.3794e-01, -1.6908e-01,\n",
      "          9.8019e-01, -1.6530e+00, -4.7640e-02, -4.9548e-01, -7.2810e-02,\n",
      "         -1.8925e-01,  1.2399e+00, -5.0163e-01,  9.2302e-01, -1.4096e+00,\n",
      "         -1.5108e-01,  9.5455e-01, -1.8297e+00, -5.7780e-01,  2.5249e-02,\n",
      "         -7.1716e-01, -9.5601e-01],\n",
      "        [ 1.8391e+00, -4.7720e-01, -4.5860e-01,  1.2831e+00,  1.3523e+00,\n",
      "          8.9821e-01, -3.7142e-01,  1.0578e+00, -5.6287e-01,  3.6546e-01,\n",
      "         -9.6565e-02, -1.8625e-01,  1.8171e-01, -1.4661e+00,  1.2852e+00,\n",
      "         -7.7639e-01, -4.1128e-01, -3.5928e+00,  2.9738e-01, -3.1616e-01,\n",
      "         -5.5662e-01, -1.3441e+00,  8.0288e-01, -2.3420e+00, -1.6174e+00,\n",
      "         -4.3777e-01, -4.6181e-01],\n",
      "        [ 1.9370e+00, -1.3274e+00, -9.9806e-01,  8.4522e-03,  1.0330e-01,\n",
      "         -1.0480e+00, -1.6480e+00,  4.8319e-03,  8.5149e-01, -2.0406e+00,\n",
      "         -1.1970e+00,  7.6900e-01,  6.3260e-01,  1.2090e+00,  2.2084e-01,\n",
      "          2.3920e-01, -1.4183e+00,  1.8287e-01,  7.7119e-01,  7.3902e-01,\n",
      "         -6.0583e-02, -4.6353e-01,  6.3421e-01,  2.4579e-01,  1.2595e-01,\n",
      "         -1.6996e+00,  8.4025e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1):\n",
    "    logits = xenc @ weights\n",
    "    count = logits.exp()\n",
    "    print(\"count\", count.shape)\n",
    "    probs = count / count.sum(2, keepdim=True)\n",
    "    print(\"probs\", probs.shape)\n",
    "    probs = probs.sum(1)\n",
    "    print(\"probs2\", probs.shape)\n",
    "    loss = -probs[torch.arange(elements), ys].log().mean()\n",
    "\n",
    "    print(loss)\n",
    "    weights.grad = None\n",
    " \n",
    "    loss.backward()\n",
    "\n",
    "    weights.data += -50*weights.grad\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# finally, sample from the 'neural net' model\n",
    "# g = torch.Generator().manual_seed(214747)\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  while True:\n",
    "\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    # p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "    logits = xenc @ weights # predict log-counts\n",
    "    counts = logits.exp()  # counts, equivalent to N\n",
    "    # probabilities for next character\n",
    "    p = counts / counts.sum(1, keepdims=True)\n",
    "    # ----------\n",
    "\n",
    "    ix = torch.multinomial(\n",
    "        p, num_samples=1, replacement=True).item()\n",
    "    out.append(inttochar[ix])\n",
    "    if ix == 0:\n",
    "      break\n",
    "  print(''.join(out))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcac82fb26a6e7c950421e78519edb87e1a5e005e0aec2fc293e679965bb2493"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
